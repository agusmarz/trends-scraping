# Google Trends Scraper MÃ©xico - Clase Magistral de Web Scraping

Una guÃ­a completa sobre cÃ³mo construir un scraper robusto de Google Trends usando Playwright, con anÃ¡lisis profundo del proceso de debugging y resoluciÃ³n de errores.

## ğŸ“‹ Tabla de Contenidos

1. [IntroducciÃ³n](#introducciÃ³n)
2. [Â¿Por quÃ© Playwright?](#por-quÃ©-playwright)
3. [El Viaje del Debugging](#el-viaje-del-debugging)
4. [Arquitectura de la SoluciÃ³n](#arquitectura-de-la-soluciÃ³n)
5. [GuÃ­a de InstalaciÃ³n](#guÃ­a-de-instalaciÃ³n)
6. [Ejecutar el Scraper](#ejecutar-el-scraper)
7. [Supabase vs Playwright](#supabase-vs-playwright)
8. [GitHub Actions: AutomatizaciÃ³n Recurrente](#github-actions-automatizaciÃ³n-recurrente)
9. [Troubleshooting](#troubleshooting)

---

## IntroducciÃ³n

Google Trends es una herramienta poderosa que muestra quÃ© estÃ¡ buscando la gente en tiempo real. Sin embargo, no ofrece una API pÃºblica directa para obtener datos programÃ¡ticamente. Este proyecto demuestra cÃ³mo extraer datos de Google Trends MÃ©xico usando tÃ©cnicas modernas de web scraping.

**Objetivo:** Obtener las 20-25 tendencias en vivo de MÃ©xico con sus volÃºmenes de bÃºsqueda cada 24 horas, almacenarlas y visualizarlas.

---

## Â¿Por quÃ© Playwright?

### Las Opciones Evaluadas

| LibrerÃ­a | Pros | Contras | Uso Ideal |
|----------|------|---------|-----------|
| **requests + BeautifulSoup** | RÃ¡pido, simple, bajo overhead | No renderiza JavaScript, GET bÃ¡sicos | Sitios estÃ¡ticos HTML puro |
| **Selenium** | Maduro, mÃºltiples navegadores | Lento, complejo de configurar, mantenimiento pesado | Testing de QA, navegadores antiguos |
| **Scrapy** | Potente, framework completo | Overkill para sitios simples, curva de aprendizaje | Crawling de mÃºltiples pÃ¡ginas a escala |
| **Playwright** âœ… | RÃ¡pido, async, moderno, menos detectable | Requiere mÃ¡s recursos que requests | **Sitios con JavaScript pesado como Google Trends** |
| **Puppeteer** | Excelente para Node.js | No es ideal para Python | JavaScript/Node.js |

### Por QuÃ© Elegimos Playwright

\`\`\`
Google Trends = JavaScript + Single Page Application (SPA)
\`\`\`

**La realidad:** Google Trends es una SPA (Single Page Application) construida con Angular/TypeScript. El HTML inicial NO contiene los datos de tendencias. Los datos se cargan dinÃ¡micamente despuÃ©s de que JavaScript ejecuta.

**Por ejemplo, con `requests`:**
\`\`\`python
import requests
response = requests.get('https://trends.google.com/trending?geo=MX&hours=24')
# El HTML contiene solo: <div id="root"></div>
# Los datos estÃ¡n en JavaScript ejecutado DESPUÃ‰S de cargar
\`\`\`

**Con Playwright:**
\`\`\`python
async with async_playwright() as p:
    page = await context.new_page()
    await page.goto(url)
    # AquÃ­ JavaScript ha ejecutado y el DOM estÃ¡ completo
    await page.evaluate('...')  # Ejecutamos cÃ³digo dentro del navegador
\`\`\`

### Ventajas EspecÃ­ficas de Playwright

1. **Renderizado Completo de JavaScript**
   - Espera a que Angular renderice los componentes
   - Ejecuta cÃ³digo dentro del contexto del navegador

2. **API Async/Await Moderna**
   \`\`\`python
   # Async permite mÃºltiples scrapers en paralelo
   tasks = [scrape_country(country) for country in countries]
   results = await asyncio.gather(*tasks)
   \`\`\`

3. **Menos Detectable que Selenium**
   - Playwright usa headless browsers moderno
   - Google no lo detecta tan fÃ¡cilmente como a Selenium

4. **Mejor Manejo de Timeouts**
   \`\`\`python
   await page.goto(url, wait_until='domcontentloaded', timeout=30000)
   await page.wait_for_timeout(5000)  # Esperar a JavaScript renderizar
   \`\`\`

5. **EjecuciÃ³n de JavaScript**
   \`\`\`python
   result = await page.evaluate('''() => {
       // CÃ³digo JavaScript ejecutado EN el navegador
       return document.querySelectorAll('div.mZ3RIc').length
   }''')
   \`\`\`

---

## El Viaje del Debugging

### Fase 1: El Primer Intento (FallÃ³)

\`\`\`python
# âŒ INTENTO 1: Requests + BeautifulSoup
import requests
from bs4 import BeautifulSoup

response = requests.get('https://trends.google.com/trending?geo=MX&hours=24')
soup = BeautifulSoup(response.text, 'html.parser')
trends = soup.find_all('div', class_='trend-item')  # âŒ No encuentra nada
\`\`\`

**Resultado:** 0 elementos encontrados

**Por quÃ© fallÃ³:** Google Trends carga los datos con JavaScript despuÃ©s de que `requests` recibe el HTML. Lo que recibimos es solo el contenedor vacÃ­o.

### Fase 2: Entender el Problema (InvestigaciÃ³n)

EjecutÃ© el script `test_scrape.py` que explorÃ³ selectores CSS:

\`\`\`
[v0] Selector 'div.mdl-card': 0 elementos encontrados
[v0] Selector 'div[data-cid]': 0 elementos encontrados
[v0] Selector 'a[href*="/trends/explore"]': 34 elementos encontrados
    â†’ Pero estÃ¡n OCULTOS (hidden)
\`\`\`

**Descubrimiento clave:** Los elementos existen pero estÃ¡n con `display: none`. Esto significa que el HTML tiene estructura, pero los datos visibles se generan dinÃ¡micamente.

### Fase 3: Cambiar a Playwright (SoluciÃ³n)

\`\`\`python
# âœ… INTENTO 2: Playwright con renderizado completo
async with async_playwright() as p:
    browser = await p.chromium.launch()
    page = await context.new_page()
    await page.goto(url, wait_until='domcontentloaded')
    await page.wait_for_timeout(5000)  # Esperar a JS
    
    elements = await page.query_selector_all('a[href*="/trends/explore"]')
    # Ahora tenemos 34 elementos VISIBLES
\`\`\`

**Resultado:** Elementos encontrados pero con contenido incorrecto (navegaciÃ³n UI, no datos)

### Fase 4: Inspeccionar la Estructura Real

CreÃ© `debug_trends_structure.py` que:
1. CapturÃ³ todo el texto visible
2. BuscÃ³ palabras conocidas ("leÃ³n", "monterrey", etc.)
3. AnalizÃ³ la estructura HTML alrededor de esos elementos

**Salida del debug:**
\`\`\`json
{
  "trend_name": "amÃ©rica - leÃ³n",
  "classes": ["mZ3RIc"],
  "parent_tag": "div",
  "siblings": {
    "volume": "200 mil+",
    "volume_class": "qNpYPd"
  }
}
\`\`\`

**Descubrimiento:** Los nombres estÃ¡n en `div.mZ3RIc` y los volÃºmenes en `div.qNpYPd`

### Fase 5: Implementar los Selectores Correctos

\`\`\`python
trends_data = await page.evaluate('''
    () => {
        let trends = [];
        
        // Los selectores correctos que encontramos
        const trendNames = document.querySelectorAll('div.mZ3RIc');
        const volumeElements = document.querySelectorAll('div.qNpYPd');
        
        for (let i = 0; i < Math.min(trendNames.length, volumeElements.length); i++) {
            trends.push({
                term: trendNames[i].textContent.trim(),
                volume: volumeElements[i].textContent.trim()
            });
        }
        return trends;
    }
''')
\`\`\`

**Resultado:** âœ… Extrae correctamente: "amÃ©rica - leÃ³n", "carlos manzo", "monterrey - tigres", etc.

### Lecciones Aprendidas

1. **Inspecciona siempre el DOM renderizado**, no solo el HTML inicial
2. **Los selectores cambian con frecuencia** - mantÃ©n mÃºltiples fallbacks
3. **JavaScript es tu aliado** - ejecuta cÃ³digo dentro del navegador
4. **Debugging progresivo** - confirma cada paso antes de avanzar

---

## Arquitectura de la SoluciÃ³n

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  scrape_trends.py (Script Principal)    â”‚
â”‚  - Inicia Playwright                    â”‚
â”‚  - Navega a Google Trends MÃ©xico        â”‚
â”‚  - Espera renderizado de JavaScript     â”‚
â”‚  - Extrae datos con selectores CSS      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  trends_data.json   â”‚
        â”‚ (Datos extraÃ­dos)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  trends.html (Frontend)          â”‚
    â”‚  - Carga JSON                    â”‚
    â”‚  - Renderiza tabla de tendencias â”‚
    â”‚  - Muestra grÃ¡ficos              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### Flujo de EjecuciÃ³n

\`\`\`python
1. async def scrape_google_trends_mexico()
   â†“
2. async_playwright() # Iniciar navegador
   â†“
3. page.goto() # Navegar a URL
   â†“
4. page.wait_until='domcontentloaded' # Esperar DOM bÃ¡sico
   â†“
5. await page.wait_for_timeout(5000) # Esperar JavaScript
   â†“
6. page.evaluate() # Ejecutar cÃ³digo en el navegador
   â†“
7. Retornar JSON con datos
   â†“
8. Guardar en trends_data.json
\`\`\`

---

## GuÃ­a de InstalaciÃ³n

### Requisitos Previos

- Python 3.7+
- pip (gestor de paquetes)

### Pasos de InstalaciÃ³n

\`\`\`bash
# 1. Clonar o descargar el proyecto
cd Scraping_pytrends

# 2. Instalar dependencias Python
pip install playwright

# 3. Instalar navegadores Playwright
playwright install chromium

# 4. Verificar instalaciÃ³n
python -c "import playwright; print('âœ… Playwright instalado')"
\`\`\`

### Estructura de Carpetas

\`\`\`
Scraping_pytrends/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ scrape_trends.py          # Script principal
â”‚   â”œâ”€â”€ debug_trends_structure.py # Script de debugging
â”‚   â””â”€â”€ test_scrape.py            # Script de pruebas
â”œâ”€â”€ public/
â”‚   â””â”€â”€ trends.html               # Frontend
â”œâ”€â”€ trends_data.json              # Datos generados
â”œâ”€â”€ README.md                     # Este archivo
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ scrape.yml            # GitHub Actions
\`\`\`

---

## Ejecutar el Scraper

### EjecuciÃ³n Manual

\`\`\`bash
# Ejecutar el scraper
python scripts/scrape_trends.py

# Resultado esperado:
# [v0] Navegando a Google Trends MÃ©xico...
# [v0] DOM cargado. Esperando a que JavaScript renderice...
# [v0] Extrayendo tendencias del DOM...
# [v0] Tendencias extraÃ­das: 20
# [v0] Top 5 tendencias:
#   1. amÃ©rica - leÃ³n (volumen: 200 mil+)
#   2. carlos manzo (volumen: 200 mil+)
# ...
# [v0] Datos guardados en trends_data.json
\`\`\`

### Ver el Frontend

\`\`\`bash
# Abrir el archivo HTML en navegador
# Windows:
start public/trends.html

# Mac:
open public/trends.html

# Linux:
xdg-open public/trends.html
\`\`\`

### Salida (trends_data.json)

\`\`\`json
{
  "timestamp": "2025-11-02T10:30:45.123456",
  "country": "MÃ©xico",
  "geo_code": "MX",
  "timeframe": "Ãšltimas 24 horas",
  "total_trends": 20,
  "trends": [
    {
      "rank": 1,
      "term": "amÃ©rica - leÃ³n",
      "volume": 100,
      "volume_text": "200 mil+"
    },
    {
      "rank": 2,
      "term": "carlos manzo",
      "volume": 100,
      "volume_text": "200 mil+"
    }
  ],
  "source": "Google Trends (Scraping Real)",
  "status": "success"
}
\`\`\`

---

## Supabase vs Playwright

### Â¿QuÃ© es Supabase?

**Supabase** es una **base de datos en la nube** (Backend-as-a-Service) basada en PostgreSQL.

\`\`\`
Supabase â‰ˆ Firebase (Google) pero open-source + PostgreSQL
\`\`\`

### Diferencias Fundamentales

| Aspecto | Playwright | Supabase |
|--------|-----------|----------|
| **Tipo** | Web Scraping Tool | Base de Datos |
| **FunciÃ³n** | Renderizar navegadores y extraer datos | Almacenar datos persistentemente |
| **EjecuciÃ³n** | En la mÃ¡quina del cliente/servidor | En servidor remoto (nube) |
| **Usa para** | Obtener datos de sitios web | Guardar datos extraÃ­dos |
| **Lenguaje** | Python (con API en JS/Python) | SQL (acceso via REST API) |

### AnalogÃ­a: Pizza

\`\`\`
Playwright = El repartidor que va y RECOGE la pizza del restaurante
Supabase = El refrigerador de tu casa donde ALMACENAS la pizza
\`\`\`

### Â¿CÃ³mo Funcionan Juntos?

\`\`\`python
# Paso 1: Playwright EXTRAE datos
trends = await scrape_google_trends_mexico()
# Resultado: {"trends": [...]}

# Paso 2: Supabase ALMACENA datos
supabase_client.table('trends').insert({
    'timestamp': trends['timestamp'],
    'data': trends['trends'],
    'country': 'MX'
})
\`\`\`

### Por QuÃ© Necesitas Ambos

**Playwright solo:** Extraes datos, pero se pierden si apagas la computadora
\`\`\`python
data = scrape_trends()  # Obtengo datos
# Si cierro la app, Â¿dÃ³nde estÃ¡n los datos?
\`\`\`

**Playwright + Supabase:** Extraes datos y los almacenas permanentemente
\`\`\`python
data = scrape_trends()           # Obtengo datos (Playwright)
store_to_database(data)          # Los almaceno (Supabase)
# Puedo acceder a ellos meses despuÃ©s
\`\`\`

### Ejemplo PrÃ¡ctico

**Caso de Uso:** Queremos ver cÃ³mo han cambiado las tendencias en los Ãºltimos 30 dÃ­as.

1. **Con solo Playwright:**
   \`\`\`python
   data_hoy = scrape_trends()  # Obtengo hoy
   # Â¿Y los datos de ayer, hace una semana, hace un mes?
   # Se perdieron porque no hay almacenamiento
   \`\`\`

2. **Con Playwright + Supabase:**
   \`\`\`python
   data_hoy = scrape_trends()
   db.insert(data_hoy)           # Guardar en Supabase
   
   # Luego puedo hacer queries:
   db.table('trends')
     .select('*')
     .where('date', '>=', '2025-10-02')
     .execute()
   # Resultado: tendencias de los Ãºltimos 30 dÃ­as
   \`\`\`

---

## GitHub Actions: AutomatizaciÃ³n Recurrente

GitHub Actions te permite ejecutar scripts automÃ¡ticamente en servidores de GitHub sin tener que dejar tu computadora prendida.

### Plan Completo de ImplementaciÃ³n

#### Paso 1: Crear Archivo de Workflow

Crea: `.github/workflows/scrape.yml`

\`\`\`yaml
name: Google Trends Scraper

on:
  # Ejecutar cada 24 horas
  schedule:
    - cron: '0 0 * * *'  # 00:00 UTC (18:00 CDMX)
  
  # TambiÃ©n permitir ejecuciÃ³n manual
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install playwright
          playwright install chromium
      
      - name: Run scraper
        run: python scripts/scrape_trends.py
      
      - name: Upload data to repository
        run: |
          git config --local user.email "bot@github.com"
          git config --local user.name "GitHub Bot"
          git add trends_data.json
          git commit -m "Update trends data - $(date)" || echo "No changes"
          git push
      
      - name: Upload to Supabase
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: python scripts/upload_to_supabase.py

\`\`\`

#### Paso 2: Agregar Secretos en GitHub

En tu repositorio:
1. Ve a **Settings** â†’ **Secrets and variables** â†’ **Actions**
2. Agrega:
   - `SUPABASE_URL`: Tu URL de Supabase
   - `SUPABASE_KEY`: Tu API key de Supabase

#### Paso 3: Script para Supabase (Opcional)

Crea: `scripts/upload_to_supabase.py`

\`\`\`python
import json
import os
from datetime import datetime
from supabase import create_client, Client

# Leer datos generados
with open('trends_data.json', 'r') as f:
    trends_data = json.load(f)

# Conectar a Supabase
url = os.getenv('SUPABASE_URL')
key = os.getenv('SUPABASE_KEY')
supabase: Client = create_client(url, key)

# Insertar datos
response = supabase.table('trends').insert({
    'timestamp': trends_data['timestamp'],
    'country': 'MX',
    'total_trends': trends_data['total_trends'],
    'data': json.dumps(trends_data['trends']),
}).execute()

print(f"Datos guardados en Supabase: {response}")
\`\`\`

#### Paso 4: Crear Tabla en Supabase

En la consola de Supabase, ejecuta:

\`\`\`sql
CREATE TABLE trends (
  id BIGINT PRIMARY KEY GENERATED ALWAYS AS IDENTITY,
  timestamp TIMESTAMP DEFAULT NOW(),
  country VARCHAR(10),
  total_trends INT,
  data JSONB,
  created_at TIMESTAMP DEFAULT NOW()
);

-- Ãndice para queries rÃ¡pidas
CREATE INDEX idx_trends_timestamp ON trends(timestamp);
\`\`\`

#### Paso 5: Flujo Completo Automatizado

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GitHub Actions Timer (Cron)            â”‚
â”‚  Ejecuta cada 24 horas automÃ¡ticamente  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Servidor de GitHub    â”‚
    â”‚  Ejecuta scraper       â”‚
    â”‚  python scrape_trends  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ trends_data.json         â”‚
    â”‚ (Datos generados)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
      â–¼             â–¼
  GitHub Repo  Supabase (DB)
  (Historial)  (Almacenamiento)
\`\`\`

### Ventajas de Este Setup

1. âœ… **Automatizado**: Se ejecuta solo cada 24 horas
2. âœ… **Sin dependencia de tu computadora**: Corre en servidores de GitHub
3. âœ… **Historial**: Todos los datos guardados en Supabase
4. âœ… **Gratuito**: GitHub Actions te da 2000 minutos/mes gratis
5. âœ… **Escalable**: Puedes agregar mÃ¡s paÃ­ses/fuentes fÃ¡cilmente

### Monitoreo

Ve a **Actions** en tu repositorio para ver:
- âœ… Ejecuciones exitosas
- âŒ Errores
- â±ï¸ DuraciÃ³n de ejecuciÃ³n
- ğŸ“Š Historial de runs

---

## Troubleshooting

### Problema: "TimeoutError: Page.wait_for_selector exceeded"

**Causa:** Google Trends tardÃ³ mÃ¡s de 15 segundos en cargar

**SoluciÃ³n:**
\`\`\`python
# Aumentar timeout en scrape_trends.py
await page.goto(url, wait_until='domcontentloaded', timeout=60000)  # 60 segundos
await page.wait_for_timeout(10000)  # 10 segundos extra
\`\`\`

### Problema: "The request failed: Google returned a response with code 404"

**Causa:** Google detectÃ³ el bot y bloqueÃ³ la IP

**SoluciÃ³n:**
\`\`\`python
# Agregar user-agent realista (ya estÃ¡ en el cÃ³digo)
user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'

# O usar proxy si continÃºa siendo bloqueado
\`\`\`

### Problema: Cero tendencias extraÃ­das

**Causa:** Google cambiÃ³ las clases CSS de su HTML

**SoluciÃ³n:**
1. Ejecuta `python scripts/debug_trends_structure.py`
2. Busca las nuevas clases en `debug_structure.json`
3. Actualiza los selectores en `scrape_trends.py`

\`\`\`python
# Ejemplo: si encontraste nueva clase "ng-TrendItem"
trendNames = document.querySelectorAll('div.ng-TrendItem');  # Nueva clase
\`\`\`

### Problema: GitHub Actions dice "No such file or directory"

**Causa:** Las rutas del archivo son incorrectas

**SoluciÃ³n:** Usa rutas relativas correctas
\`\`\`yaml
run: python scripts/scrape_trends.py  # âœ… Correcto
# NO: run: python ./scrape_trends.py  # âŒ Incorrecto
\`\`\`

---

## Referencias y Recursos

### DocumentaciÃ³n
- [Playwright Python Docs](https://playwright.dev/python/)
- [Google Trends](https://trends.google.com)
- [GitHub Actions Workflows](https://docs.github.com/en/actions)
- [Supabase Documentation](https://supabase.com/docs)

### Conceptos Relacionados
- **Web Scraping Ã‰tico**: Siempre revisa `robots.txt` y `Terms of Service`
- **JavaScript Rendering**: Entender SPAs es fundamental en scraping moderno
- **Async/Await**: Clave para scrapers de alto rendimiento

---

## ConclusiÃ³n

Este proyecto demuestra:

1. **AnÃ¡lisis profundo del problema** antes de empezar a codificar
2. **Debugging sistemÃ¡tico** para entender la estructura del sitio
3. **SelecciÃ³n correcta de herramientas** (Playwright para JavaScript)
4. **AutomatizaciÃ³n robusta** con GitHub Actions
5. **Persistencia de datos** con Supabase

El scraping moderno no es solo hacer requests HTTP. Requiere entender JavaScript, DOM, async/await, y arquitecturas de SPAs. Playwright es la herramienta perfecta para este trabajo.

Â¡Feliz scraping! ğŸš€
